from backend.chains import get_conversational_agent
from backend.tools import get_tools

import os
import streamlit as st

from dotenv import load_dotenv
from streamlit import session_state as sst
from langchain import callbacks, hub
from langchain.callbacks.tracers.langchain import wait_for_all_tracers
from langchain_community.callbacks import StreamlitCallbackHandler
from langchain_core.messages import (
    AIMessage,
    AIMessageChunk,
    HumanMessage,
    SystemMessage,
)
from langchain_openai import ChatOpenAI
from langsmith import Client

load_dotenv()
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "jinair"

st.set_page_config(layout="wide")
MODEL_NAME = "gpt-4o"  # "gpt-3.5-turbo-0125"
GREETING = ""
SYSTEM_MESSAGE = "ğŸ¤– ì„¸ì¼ì¦ˆë´‡ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤."


@st.cache_resource
def get_langsmith_client():
    """
    log ë¶„ì„ì„ ìœ„í•´ langsmith clientë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    return Client(api_key=os.environ["LANGCHAIN_API_KEY"])


def refresh():
    st.rerun()


@st.cache_resource
def create_conversational_agent(
    model_name="gpt-4o",
    temp=0.0,
):
    """
    ìœ ì €ê°€ ì§€ì •í•œ í”„ë¡¬í”„íŠ¸(`agent_instruct_template`)ë¥¼ ì‚¬ìš©í•˜ì—¬ GPT ê¸°ë°˜ì˜ agentë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ agentëŠ” ë©€í‹°í„´ ëŒ€í™”ë¥¼ ì²˜ë¦¬í•˜ê³ , íˆ´ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ì— ì¤‘ê´„í˜¸('{}')ê°€ ì“°ì´ëŠ” ê²½ìš°ê°€ ìˆì–´ `template_format="jinja2"`ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
    """
    sst.llm = ChatOpenAI(model_name=model_name, temperature=temp, verbose=True)
    sst.tools = get_tools()

    chain = get_conversational_agent(
        llm=sst.llm,
        tools=sst.tools,
        prompts=sst.templates,
    )

    print("ğŸš’ Chains have been newly created.")
    return chain


if "messages" not in sst:
    sst.agent = create_conversational_agent(model_name=MODEL_NAME)
    sst.messages = []
    sst.steps = []
    sst.langsmith_client = get_langsmith_client()
    sst.templates = {}

# ì¶œë ¥ë˜ëŠ” ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
st.markdown(
    """
<style>
img {
    max-height: 150px;
}
</style>
""",
    unsafe_allow_html=True,
)
st.title("JinAir ì„¸ì¼ì¦ˆë´‡ ğŸ¤–")

if "agent" in sst:
    sst.messages.append(SystemMessage(content=SYSTEM_MESSAGE))

    with st.chat_message("ai"):
        st.markdown(GREETING)

# ëŒ€í™” ë‚´ì—­ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
for i, message in enumerate(sst.messages):
    role = "human" if isinstance(message, HumanMessage) else "ai"
    with st.chat_message(role):
        st.markdown(message.content)

        if role == "ai":
            with st.expander("ğŸ¤– ë‚´ë¶€ ë‹¨ê³„", expanded=False):
                st.write(sst.steps[i])

if prompt := st.chat_input(""):
    with st.chat_message("human"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        st_callback = StreamlitCallbackHandler(st.container())
        sst.reply_placeholder = st.empty("")
        outputs = sst.agent.stream(
            {
                "user_query": prompt,
                "chat_history": sst.messages,
            },
            config={
                "callbacks": [st_callback],
            },
        )
        answer = ""
        for output in outputs:
            # pprint(output)
            for msg in output["messages"]:
                # tool ì‚¬ìš© ì „, í›„ì˜ AI ë©”ì‹œì§€ í†µí•©
                if isinstance(msg, (AIMessageChunk, AIMessage)):
                    msg_chunk = msg.content
                    if not answer.endswith(msg_chunk):
                        answer += "\n" + (msg.content)
                        sst.reply_placeholder.markdown(answer)

            if "intermediate_steps" in output:
                intermediate = output["intermediate_steps"]

        final_answer = answer

    sst.messages.append(HumanMessage(content=prompt))
    sst.messages.append(AIMessage(content=final_answer))
    sst.steps.append(None)
    sst.steps.append(intermediate)
